{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. RNN vs Modern Models (Transformers)",
    "",
    "## Limitations of RNNs",
    "1. **Sequential Computation**: Cannot parallelize training. Slow on long sequences.",
    "2. **Long-term Dependencies**: Even with LSTM, very long context is hard to capture.",
    "",
    "## Valid Use Cases for RNNs Today",
    "- Streaming data (low latency, one token at a time).",
    "- Limited compute resources (edge devices).",
    "- Simple time-series forecasting.",
    "",
    "## The Rise of Transformers",
    "**Attention is All You Need** (2017) changed everything. Transformers process the entire sequence at once (Self-Attention).",
    "",
    "## Comparison Table",
    "| Feature | RNN/LSTM | Transformer |",
    "|---------|----------|-------------|",
    "| Training Parallelism | No (Sequential) | Yes |",
    "| Long Context | Struggling | Excellent (Attention) |",
    "| Compute Cost | Low/Medium | High (Quadratic with len) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}