{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Engr.Muhammad Javed***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Forward and Backward Propagation in RNN\n",
    "\n",
    "## Forward Propagation\n",
    "As seen in the previous notebook, forward propagation is just looping through time steps and updating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)\n",
    "Training an RNN is similar to training a traditional Neural Network, but with a twist. Because the parameters are shared across all time steps, the gradient at each output depends not only on the current time step but also on previous time steps.\n",
    "\n",
    "This is called **Backpropagation Through Time (BPTT)**. It's essentially unrolling the RNN for all time steps and then using standard backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "The total loss $L$ is the sum of the losses at each time step $t$:\n",
    "\n",
    "$$L = \\sum_t L_t$$\n",
    "\n",
    "where $L_t$ is typically Cross-Entropy Loss for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# RNN Forward and Backward Pass Implementation\n",
    "class RNN:\n",
    "    def __init__(self, vocab_size, hidden_size, seq_length, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Model parameters\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "    def lossFun(self, inputs, targets, hprev):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        \n",
    "        # Forward Pass\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][inputs[t]] = 1 # One-hot encoding\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            ps[t] = softmax(ys[t])\n",
    "            loss += -np.log(ps[t][targets[t], 0])\n",
    "            \n",
    "        # Backward Pass\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "            \n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # Gradient Clipping\n",
    "            \n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates the core mechanics of how gradients flow back through time steps (`reversed(range(len(inputs)))`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
